import os
import joblib
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix
)

# ======================================
# ğŸ”¹ HÃ€M ÄÃNH GIÃ Má»˜T MÃ” HÃŒNH
# ======================================
def evaluate_model(model_path, X_test, y_test):
    """Load mÃ´ hÃ¬nh .pkl vÃ  Ä‘Ã¡nh giÃ¡ trÃªn táº­p test"""

    # ğŸ”¥ Chuáº©n hoÃ¡ dá»¯ liá»‡u test
    X_test = X_test.astype(str).fillna("")

    model = joblib.load(model_path)

    # Predict
    y_pred = model.predict(X_test)

    # ğŸ”¹ Chuyá»ƒn y_pred vá» cÃ¹ng kiá»ƒu vá»›i y_test
    if y_test.dtype.kind in 'if':  # int or float
        y_pred = pd.Series(y_pred).astype(float if y_test.dtype.kind=='f' else int)
    else:
        y_pred = pd.Series(y_pred).astype(str)

    # ğŸ”¥ TÃ¬m pos_label tá»± Ä‘á»™ng (chá»‰ cáº§n khi dÃ¹ng precision/recall/f1)
    unique_labels = sorted(set(y_test.unique()))
    pos_label = unique_labels[-1]

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, pos_label=pos_label, zero_division=0)
    rec = recall_score(y_test, y_pred, pos_label=pos_label, zero_division=0)
    f1 = f1_score(y_test, y_pred, pos_label=pos_label, zero_division=0)

    return acc, prec, rec, f1, y_pred

# ======================================
# ğŸ”¹ Váº¼ BIá»‚U Äá»’
# ======================================
def plot_confusion_matrix(y_true, y_pred, model_name, save_path):
    cm = confusion_matrix(y_true, y_pred, labels=sorted(y_true.unique()))
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=sorted(y_true.unique()),
                yticklabels=sorted(y_true.unique()))
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

def plot_model_scores(result_df, save_path):
    plt.figure(figsize=(10, 6))
    result_df_plot = result_df.set_index("model")[["accuracy", "precision", "recall", "f1_score"]]
    result_df_plot.plot(kind="bar", figsize=(12, 6))
    plt.title("So sÃ¡nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh")
    plt.ylabel("Score (0 â†’ 1)")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

# ======================================
# ğŸ”¹ MAIN ENTRY POINT
# ======================================
def main(model_dir="model/", test_path="data/test.csv", result_dir="result/"):
    print("ğŸš€ Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test...")

    # ğŸ”¹ Táº¡o thÆ° má»¥c result vÃ  plots náº¿u chÆ°a tá»“n táº¡i
    os.makedirs(result_dir, exist_ok=True)
    os.makedirs(os.path.join(result_dir, "plots"), exist_ok=True)

    # Äá»c táº­p test
    if not os.path.exists(test_path):
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file test: {test_path}")

    df_test = pd.read_csv(test_path)
    df_test["text"] = df_test["text"].astype(str)
    df_test = df_test[df_test["text"].str.strip() != ""]
    df_test = df_test.dropna(subset=["text", "label"])
    df_test.reset_index(drop=True, inplace=True)

    X_test, y_test = df_test["text"], df_test["label"]

    model_files = [f for f in os.listdir(model_dir) if f.endswith(".pkl")]
    if not model_files:
        raise FileNotFoundError("âš ï¸ KhÃ´ng tÃ¬m tháº¥y mÃ´ hÃ¬nh nÃ o trong thÆ° má»¥c model/")

    results = []

    for model_file in model_files:
        model_path = os.path.join(model_dir, model_file)
        print(f"\nğŸ§  Äang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh: {model_file}")

        acc, prec, rec, f1, y_pred = evaluate_model(model_path, X_test, y_test)

        print(f"ğŸ“Š Accuracy : {acc:.4f}")
        print(f"ğŸ“Š Precision: {prec:.4f}")
        print(f"ğŸ“Š Recall   : {rec:.4f}")
        print(f"ğŸ“Š F1-score : {f1:.4f}")

        results.append({
            "model": model_file.replace(".pkl", ""),
            "accuracy": acc,
            "precision": prec,
            "recall": rec,
            "f1_score": f1
        })

        # ğŸ”¹ Váº½ confusion matrix cho tá»«ng mÃ´ hÃ¬nh
        cm_path = os.path.join(result_dir, "plots", f"cm_{model_file.replace('.pkl','')}.png")
        plot_confusion_matrix(y_test, y_pred, model_file, cm_path)

    # ğŸ”¹ LÆ°u káº¿t quáº£ CSV
    result_df = pd.DataFrame(results)
    csv_path = os.path.join(result_dir, "evaluation_results.csv")
    result_df.to_csv(csv_path, index=False)
    print("\nâœ… ÄÃ£ lÆ°u káº¿t quáº£ táº¡i:", csv_path)

    # ğŸ”¹ Váº½ biá»ƒu Ä‘á»“ tá»•ng há»£p
    plot_score_path = os.path.join(result_dir, "plots", "model_scores.png")
    plot_model_scores(result_df, plot_score_path)
    print("ğŸ“ˆ ÄÃ£ táº¡o biá»ƒu Ä‘á»“ tá»•ng há»£p táº¡i:", plot_score_path)

    # ğŸ”¹ In báº£ng tá»•ng káº¿t
    print("\nğŸ¯ Báº¢NG Tá»”NG Káº¾T Káº¾T QUáº¢:")
    print(result_df.sort_values(by="f1_score", ascending=False).to_string(index=False))


if __name__ == "__main__":
    main()
