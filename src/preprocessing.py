import os
import re
import string
import pandas as pd
from pathlib import Path
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split

STOP_WORDS = set(stopwords.words("english"))
LEMMATIZER = WordNetLemmatizer()

# =============================
# ğŸ”¹ HÃ€M LÃ€M Sáº CH Ná»˜I DUNG
# =============================
def clean_text(text: str) -> str:
    """LÃ m sáº¡ch vÄƒn báº£n: bá» kÃ½ tá»± Ä‘áº·c biá»‡t, lowercase, stopwords, lemmatization"""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"http\S+", " ", text)           # bá» link
    text = re.sub(f"[{string.punctuation}]", " ", text)  # bá» dáº¥u cÃ¢u
    text = re.sub(r"\d+", " ", text)               # bá» sá»‘
    text = re.sub(r"\s+", " ", text).strip()       # bá» khoáº£ng tráº¯ng thá»«a

    words = [LEMMATIZER.lemmatize(w) for w in text.split() if w not in STOP_WORDS]
    return " ".join(words)

# =============================
# ğŸ”¹ TIá»€N Xá»¬ LÃ Dá»® LIá»†U CHÃNH
# =============================
def preprocess_data(input_path="data/spam.csv", output_path="data/preprocessed.csv"):
    """
    Äá»c dá»¯ liá»‡u tá»« file CSV (spam/ham vÃ  text),
    lÃ m sáº¡ch text, mÃ£ hÃ³a nhÃ£n, vÃ  lÆ°u láº¡i.
    """
    print("ğŸ§¹ Äang tiá»n xá»­ lÃ½ dá»¯ liá»‡u...")

    # Äá»c dá»¯ liá»‡u gá»‘c
    df = pd.read_csv(input_path)
    df.columns = df.columns.str.lower()  # chuáº©n hÃ³a tÃªn cá»™t

    # Náº¿u file khÃ´ng cÃ³ header
    if "label" not in df.columns or "text" not in df.columns:
        df = pd.read_csv(input_path, names=["label", "text"], header=None)

    # Chuáº©n hÃ³a nhÃ£n
    df["label"] = df["label"].map(
        {"spam": 1, "ham": 0, "non-spam": 0, "not spam": 0, "legit": 0}
    ).fillna(df["label"])

    # LÃ m sáº¡ch text
    df["text"] = df["text"].astype(str).apply(clean_text)

    # Bá» giÃ¡ trá»‹ rá»—ng vÃ  trÃ¹ng láº·p
    df = df.dropna().drop_duplicates()

    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
    Path(os.path.dirname(output_path)).mkdir(parents=True, exist_ok=True)

    # LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
    df.to_csv(output_path, index=False)
    print(f"âœ… Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ vÃ  lÆ°u táº¡i: {output_path}")
    print(f"ğŸ“Š Tá»•ng sá»‘ máº«u sau xá»­ lÃ½: {len(df)}")

    return df

# =============================
# ğŸ”¹ HÃ€M CHIA Dá»® LIá»†U
# =============================
def split_train_test(df, train_size=0.8):
    from sklearn.model_selection import train_test_split
    df_train, df_test = train_test_split(
        df, test_size=1 - train_size, stratify=df["label"], random_state=42
    )

    df_train.to_csv("data/train.csv", index=False)
    df_test.to_csv("data/test.csv", index=False)
    print(f"ğŸ“š Train: {len(df_train)} | Test: {len(df_test)}")
    return df_train, df_test



# =============================
# ğŸ”¹ MAIN ENTRY POINT
# =============================
if __name__ == "__main__":
    # 1ï¸âƒ£ Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
    df_clean = preprocess_data()

    # 2ï¸âƒ£ Chia train/val/test
    split_train_test(df_clean)
